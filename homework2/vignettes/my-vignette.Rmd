---
title: "homework2"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{my-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(homework2)
```

#problem 1

$$
y = \beta_0 + \beta_1x\\

min\ (y - \beta x)'(y - \beta x)\\
=y'y - \beta' xy - y'\beta x + \beta' x'x\beta\\
=y'y - 2\beta' x'y + \beta' x'x\beta\\

\frac{\partial f}{\partial \beta} = -2x'y + 2x'x\beta\\
set\ to\ zero\\
-2x'y + 2x'x\hat\beta = 0\\
therefore ,\ the ordinary\ least\ squared\ estimator\
\hat\beta = (x'x)^{-1}x'y\\
x'x = \begin{bmatrix} 
1&...&1 \\
x_1&...&x_n
\end{bmatrix}
\begin{bmatrix} 
1&x_1 \\
.&.\\
.&.\\
.&.\\
1&x_n
\end{bmatrix} 
= 
\begin{bmatrix} 
n&n\bar x\\
n\bar x&\sum_{i=1}^{n}x_i^2
\end{bmatrix}\\

So\ the\ inverse\ of\ x'x = \frac{1}{n\sum_{i=1}^{n}x_i^2 - n\bar x^2}
\begin{bmatrix} 
\sum_{i=1}^{n}x_i^2&-n\bar x\\
-n\bar x&n\\
\end{bmatrix}\\


x'y = \begin{bmatrix} 
1&...&1 \\
x_1&...&x_n
\end{bmatrix}
\begin{bmatrix} 
y_1 \\
.\\
.\\
.\\
y_n
\end{bmatrix}
= \begin{bmatrix} 
n \bar y \\
\sum_{i=1}^{n}x_iy_i
\end{bmatrix}\\

\hat \beta = \frac{1}{n\sum_{i=1}^{n}x_i^2 - n\bar x^2}
\begin{bmatrix} 
\sum_{i=1}^{n}x_i^2&-n\bar x\\
-n\bar x&n\\
\end{bmatrix}
\begin{bmatrix} 
n \bar y \\
\sum_{i=1}^{n}x_iy_i
\end{bmatrix}\\
= \frac{1}{\sum_{i=1}^{n}(x_i - \bar x)^2}
\begin{bmatrix} 
\bar y\sum_{i=1}^{n}x_i^2 - \bar x\sum_{i=1}^{n}x_iy_i\\
\sum_{i=1}^{n}x_iy_i-n \bar x\bar y
\end{bmatrix}\\

\hat \beta_1 = \frac{1}{\sum_{i=1}^{n}(x_i - \bar x)^2} * \bar y(\sum_{i=1}^{n}x_i^2 - \bar x^2) - \bar x(\sum_{i=1}^{n}x_iy_i - \bar y \bar x)\\
= \frac{\sum_{i=1}^{n}(x_i- \bar x)(y_i - \bar y)}{(\sum_{i=1}^{n}x_i - \bar x)^2}\\

\hat \beta_0 = y - \beta_1 \bar x
$$

#problem 4
Section 2.8 of CASL shows that as the numerical stability decreases, statistical errors increase. Reproduce
the results and then show that using ridge regression can increase numerical stability and decrease
statistical error.

A vector $\beta$ is constructed with first coordinate equal to 1 and the other 24 coorrdinates being 0s. A data matrix X is generated by randomly sampling normal variables. The condition number, indicating numerical stablity, is calculated.
```{r}
n <- 1000; p <- 25
beta <- c(1, rep(0, p - 1))
X <- matrix(rnorm(n * p), ncol = p)
svals <- svd(crossprod(X))$d
max(svals) / min(svals)
```

The l2-morm assesses how close the ordinary least squares estimate is to the true $\beta$. The mean error rate is reported .
```{r}

casl_ols_svd <-
function(X, y)
{
svd_output <- svd(X)
r <- sum(svd_output$d > .Machine$double.eps)
U <- svd_output$u[, 1:r]
V <- svd_output$v[, 1:r]
beta <- V %*% (t(U) %*% y / svd_output$d[1:r]) 
beta
}

N <- 1e4
l2_errors <- rep(0, N) 
for (k in 1:N) {
y <- X %*% beta + rnorm(n) 
betahat <- casl_ols_svd(X, y)
l2_errors[k] <- sqrt(sum((betahat - beta)^2)) }
mean(l2_errors)
```

The we use ridge regression to assess numerical stability and statistical error and a decreasing condition number and a decreasing error rate is obtained, indicating ridge regression could increase numerical stablity and statistical error.

```{r}
svals <- svd(crossprod(X) + diag(rep(0.1, ncol(X))))$d
max(svals) / min(svals)
```

```{r}

newl2_errors <- rep(0, N) 
for (k in 1:N) {
y <- X %*% beta + rnorm(n) 
betahat <- solve( crossprod(X) + diag(rep(30, ncol(X))) ) %*% t(X) %*% y
newl2_errors[k] <- sqrt(sum((betahat - beta)^2)) }
mean(newl2_errors)
```
Then we replace the first column of X with a linear combination of the originial first column and the second column, therefore, two columns of X become highly correlated and condition number increases, indicating decreasing numerical stablity.

```{r}
alpha <- 0.001
X[,1] <- X[,1] * alpha + X[,2] * (1 - alpha) 
svals <- svd(crossprod(X))$d
max(svals) / min(svals)
```

And the error rate also increases.
```{r}
N <- 1e4; l2_errors <- rep(0, N)
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- solve(crossprod(X), crossprod(X, y)) 
l2_errors[k] <- sqrt(sum((betahat - beta)^2))
} 
mean(l2_errors)
```

Again, we use ridge regression and observe both condition number and error rate decreasing, indicating increasing numerical stability and statistical error.
```{r}
svals <- svd(crossprod(X) + diag(rep(0.1, ncol(X))))$d
max(svals) / min(svals)
```
 
```{r}
newl2_errors <- rep(0, N) 
for (k in 1:N) {
y <- X %*% beta + rnorm(n) 
betahat <- solve( crossprod(X) + diag(rep(0.1, ncol(X))) ) %*% t(X) %*% y
newl2_errors[k] <- sqrt(sum((betahat - beta)^2)) }
mean(newl2_errors)
```

#problem 5

$$
LASSO\ penalty:\ \frac{1}{2n}||Y - X\beta||^2_2\ + \lambda||\beta||_1\\
in\ order\ to\ minimize\ the\ term\\
set\ -\frac{1}{n}X^T(Y-X\hat \beta) + \lambda sign(\hat \beta) = 0\\
\frac{1}{n}X^T(Y-X\hat \beta) = \lambda sign(\hat \beta)\\

\frac{1}{n}X^T - \frac{1}{n}X^TX\hat \beta = \lambda sign(\hat \beta)\\
\hat \beta^{LASSO} = \frac{1}{n}X^TY - \lambda sign(\hat \beta^{LASSO})\\
Since\
|X^T_1Y| \leq n\lambda\\
\hat \beta^{LASSO} = \lambda - \lambda sign(\hat \beta^{LASSO})\\
\hat \beta^{LASSO} = 0
$$
